
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Lu, Xudong</title>
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 850px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul {
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.bLurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 13px;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="./src/css" rel="stylesheet" type="text/css">
<script async="" src="./src/analytics.js"></script><script async="" src="./src/analytics.js"></script><script async="" src="./src/analytics(1).js"></script><script async="" src="./src/analytics(1).js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'Lucky-Lance.github.io');
  ga('send', 'pageview');

</script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');


</script><script type="text/javascript" src="./src/hidebib.js"></script><script src="./src/main.js"></script><script type="text/javascript" src="./src/jquery-1.12.4.min.js"></script><script type="text/javascript" src="./src/jquery-1.12.4(1).min.js"></script></head>

<body data-new-gr-c-s-check-loaded="14.1156.0" data-gr-ext-installed="">
<div style="margin-bottom: 1em; border-top-right-radius:10px; border-top-left-radius:10px; border-bottom-left-radius:10px; border-bottom-right-radius:10px; border: 0px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
  <img title="Xudong Lu" style="float: left; padding-left: .3em; padding-top: 0em; height: 140px;" src="./src/homepage.jpg">
  <div style="padding-left: 12em; padding-top: 0em; vertical-align: top; height: 120px; width: 100%;">
    <p>&nbsp;</p>
    <p><span style="line-height: 80%; font-size: 16pt;">Xudong Lu (ÈôÜÂæê‰∏ú)</span></p>
    <p>&nbsp;</p>
    <p>Multimedia Laboratory (MMLab)</p>
    <p>The Chinese University of Hong Kong</p>
    <p>Email: <a href="mailto:luxudong@link.cuhk.edu.hk" target="_blank">luxudong@link.cuhk.edu.hk</a>, <a href="mailto:luxudong2001@sjtu.edu.cn" target="_blank">luxudong2001@sjtu.edu.cn</a></p>
    <p>&nbsp;</p>
    <p>
    <a href="https://scholar.google.com/citations?user=G9jWIggAAAAJ&hl=zh-CN" target="_blank"><img class="responsive-img social-photo " style="float: left; padding-left: 0.3em; padding-top: 0em; height: 30px; " src="./src/google_color1.png"></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </p>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2>About Me
</h2>

<br>

<div class="paper">

I am currently a third-year Ph.D. student at the <a href="https://mmlab.ie.cuhk.edu.hk/" target="_blank"> Multimedia Laboratory (MMLab)</a>, <a href="https://cuhk.edu.hk/english/index.html" target="_blank">The Chinese University of Hong Kong</a>. My research advisor is <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Prof. Hongsheng Li</a>.

<br> <br>

I received the B. E. degree from the <a href="https://cs.sjtu.edu.cn/" target="_blank">Department of Computer Science and Engineering</a>, <a href="https://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a> in 2023, ranking <b>2/119</b> in the major, and awarded with the <a href="http://www.moe.gov.cn/jyb_xwfb/xw_zt/moe_357/jyzt_2015nztzl/2015_zt06/15zt06_gxzzzc/gxzz_bzks/201508/t20150810_199202.html" target="_blank">National Scholarship</a> (top 0.2%) in 2020, 2021 and 2022.

<br> <br>

During my undergraduate studies, from 2020 to 2022, I worked closely with <a href="https://mvig-rhos.com/" target="_blank">Prof. Yonglu Li</a> and <a href="https://www.mvig.org/" target="_blank">Prof. Cewu Lu</a>, focusing on human activity understanding. From 2022 to 2023, I worked with <a href="https://thinklab.sjtu.edu.cn/" target="_blank">Prof. Junchi Yan</a> on AI for Science (AI4Science). My current research interests include model compression, large language models, and multimodal large language models.

<br><br>

</div>
</div>
</div>

<h2>News</h2><br>
<div style="height: 350px; overflow-y: scroll;">
  <div style="clear: both;">
    <div class="section">
      <div class="paper">
        <ul style="padding-left: 20px; margin-left: 0; list-style-position: inside;">
    <li> June 2025: One paper on MLLM (first author) is accepted by ICCV 2025.</li>
    <li> May 2025: One paper on oriented object detection is accepted by IJCV (IF 11.6, 2024).</li>
    <li> February 2025: Two papers on MLLMs (one first author) are accepted by CVPR 2025.</li>
    <li> February 2025: Two papers (one on KV Cache pruning, another on autoformalization) are accepted by ICLR 2025 <a style="color: red;">(both Spotlight).</a></li>
    <li> November 2024: We are happy to introduce BlueLM-V-3B, an algorithm and system co-design method for multimodal large language models on mobile devices.üî•üî•üî•</li>
    <li> July 2024: One paper on video generation is accepted by ECCV 2024 <a style="color: red;">(Oral).</a></li>
    <li> May 2024: One paper (first author) on MoE LLM pruning is accepted by ACL 2024.</li>
    <li> May 2024: One paper (first author) on LLaMA-alike LLM pruning is accepted by ICML 2024.</li>
    <li> February 2024: One paper on Human Action Understanding is accepted by CVPR 2024<a style="color: red;"> (Highlight).</a></li>
    <li> August 2023: I am joining the MMLab of CUHK as a Ph.D. student.</li>
    <li> June 2023: I am honored to have the opportunity to <a href="https://news.sjtu.edu.cn/zhxw/20230623/185088.html" target="_blank">speak as the student representative</a> at the undergraduate commencement ceremony of the college.</li>
    <li> April 2023: Two papers (one first author) on Quantum Machine Learning are accepted by ICML 2023.</li>
    <li> March 2023: I am awarded with HKPFS (Hong Kong PhD Fellowship Scheme).</li>
        </ul>
      </div>
    </div>
  </div>
</div>

<br>

<div style="clear: both;">
  <div class="section">
  <h2 id="confpapers">Preprint</h2> 
  </div>

<div class="paper" id="paper"><img class="paper" src="./src/cdt.png" title="Rethinking Video Tokenization: A Conditioned Diffusion-based Approach">
  <div> <strong>Rethinking Video Tokenization: A Conditioned Diffusion-based Approach
  </strong><br>
  Nianzu Yang*, Pandeng Li*, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, <b>Xudong Lu</b>, Zhihang Liu, Yun Zheng, Yu Liu, <b>Junchi Yan</b><br>
  Arxiv Preprint 2025<br>
  [<a href="https://arxiv.org/abs/2503.03708" target="_blank">Paper</a>]   
    <br>
  </div>
  <div class="spanner"></div>
  </div>

<div class="paper" id="paper"><img class="paper" src="./src/smartbench.png" title="SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?">
  <div> <strong>SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?
  </strong><br>
  <b>Xudong Lu*</b>, Haohao Gao*, Renshou Wu*, Shuai Ren, Xiaoxin Chen, <b>Hongsheng Li</b>, <b>Fangyuan Li</b><br>
  Arxiv Preprint 2025<br>
  [<a href="https://arxiv.org/abs/2503.06029" target="_blank">Paper</a>]   
    <br>
  </div>
  <div class="spanner"></div>
  </div>

<div class="paper" id="paper"><img class="paper" src="./src/terdit.png" title="TerDiT: Ternary Diffusion Models with Transformers">
  <div> <strong>TerDiT: Ternary Diffusion Models with Transformers
  </strong><br>
  <b>Xudong Lu</b>, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Xue Yang, Junchi Yan, Peng Gao, <b>Hongsheng Li</b><br>
  Arxiv Preprint 2024<br>
  [<a href="https://arxiv.org/abs/2405.14854" target="_blank">Paper</a>]   
    <br>
  </div>
  <div class="spanner"></div>
  </div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Publications </h2> 
</div>

<div class="paper" id="paper"><img class="paper" src="./src/genieblue.png" title="GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices">
  <div> 
    <div style="line-height: 1.5; display: flex; align-items: center; justify-content: flex-start; position: absolute; left: 0;">
      <img src="src/jovi.png" style="height: 2em; margin-right: 5px;">
      <strong>GenieBlue: Integrating both Linguistic and Multimodal Capabilities for Large Language Models on Mobile Devices</strong>
    </div>
    <br>
  </div>
  <div>
  <br>
<b>Xudong Lu*</b>, Yinghao Chen*, Renshou Wu*, Haohao Gao, Xi Chen, Xue Yang, Xiangyu Zhao, Aojun Zhou, Fangyuan Li, Yafei Wen, Xiaoxin Chen, <b>Shuai Ren</b>, <b>Hongsheng Li</b><br>
International Conference on Computer Vision (<strong>ICCV</strong>), 2025
<br>
[<a href="https://arxiv.org/abs/2503.06019" target="_blank">Paper</a>][<a href="https://mp.weixin.qq.com/s/ztTdARR4Q0opOGP139NQkQ" target="_blank">ÈáèÂ≠ê‰Ωç (QbitAI)</a>]   
  <br>
</div>
<div class="spanner"></div>
</div>

  <div class="paper" id="paper"><img class="paper" src="./src/PointOBB-v3.png" title="PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection">
    <div> <strong>PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection
    </strong><br>
    Peiyuan Zhang*, Junwei Luo*, Xue Yang*, Yi Yu, Qingyun Li, Yue Zhou, Xiaosong Jia, <b>Xudong Lu</b>, Jingdong Chen, Xiang Li, Junchi Yan, <b>Yansheng Li</b><br>
    International Journal of Computer Vision (<strong>IJCV</strong>), 2025<br>
    [<a href="https://arxiv.org/abs/2501.13898" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>

<div class="paper" id="paper"><img class="paper" src="./src/adaptive.jpg" title="Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding">
  <div> 
      <strong>Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding</strong>
  </div>
  <div>
  Han Xiao, Yina Xie, Guanxin Tan, Yinghao Chen, Rui Hu, Ke Wang, Aojun Zhou, Hao Li, Hao Shao, <b>Xudong Lu</b>, Peng Gao, Yafei Wen, Xiaoxin Chen, Shuai Ren, <b>Hongsheng Li</b>
<br>
  The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025<br>
  [<a href="https://arxiv.org/abs/2505.05446" target="_blank">Paper</a>]
    <br>
  </div>
  <div class="spanner"></div>
  </div>



<div class="paper" id="paper"><img class="paper" src="./src/bluelm.png" title="BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices">
  <div> 
    <div style="line-height: 1.5; display: flex; align-items: center; justify-content: flex-start; position: absolute; left: 0;">
      <img src="src/jovi.png" style="height: 2em; margin-right: 5px;">
      <strong>BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices</strong>
    </div>
    <br>
  </div>
  <div>
  <br>
  <b>Xudong Lu*</b>, Yinghao Chen*, Cheng Chen*, Hui Tan*, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, Yi Zeng, Lei Wu, Liuyang Bian, Zhaoxiong Wang, Long Liu, Yanzhou Yang, Han Xiao, Aojun Zhou, Yafei Wen, Xiaoxin Chen, <b>Shuai Ren</b>, <b>Hongsheng Li</b><br>
  <a href="https://developers.vivo.com/product/ai/bluelm" target="_blank">
    <img src="src/bluelm_logo.png" style="height: 1.5em;">
</a>
  <br>
  The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025<br>
  [<a href="https://arxiv.org/abs/2411.10640" target="_blank">Paper</a>][<a href="https://mp.weixin.qq.com/s/duIfJRcABRRgmoq3J1B4ug" target="_blank">Êú∫Âô®‰πãÂøÉ (Synced China)</a>]
    <br>
  </div>
  <div class="spanner"></div>
  </div>

<div style="width: 100%; background-color: lightblue;">
  <a href="https://dev.vivo.com.cn/vdc/2024/index.html" target="_blank">
    <img class="paper" style="width: 100%;" src="./src/bluelm_v_3b_conf.jpg" title="BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices">
  </a>
</div>

  <div class="paper" id="paper"><img class="paper" src="./src/rautoformalizer.png" title="Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach">
    <div> <strong>Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach
    </strong><br>
    Qi Liu, Xinhao Zheng, <b>Xudong Lu</b>, Qinxiang Cao, <b>Junchi Yan</b>
    <br>
    International Conference on Learning Representations (<strong>ICLR</strong>), 2025, <a style="color: red;">(Spotlight)</a><br>
    [<a href="https://openreview.net/pdf?id=hUb2At2DsQ" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>

  <div class="paper" id="paper"><img class="paper" src="./src/think.png" title="ThinK: Thinner Key Cache by Query-Driven Pruning">
    <div> <strong>ThinK: Thinner Key Cache by Query-Driven Pruning
    </strong><br>
    Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, <b>Xudong Lu</b>, Aojun Zhou, Amrita Saha, Caiming Xiong, <b>Doyen Sahoo</b>
    <br>
    International Conference on Learning Representations (<strong>ICLR</strong>), 2025, <a style="color: red;">(Spotlight)</a><br>
    [<a href="https://openreview.net/pdf?id=n0OtGl6VGb" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>

  <div class="paper" id="paper"><img class="paper" src="./src/zola.png" title="ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model">
    <div> <strong>ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model
    </strong><br>
    Fu-Yun Wang, Zhaoyang Huang, Qiang Ma, Guanglu Song, <b>Xudong Lu</b>, Weikang Bian, Yijin Li, Yu Liu, <b>Hongsheng Li</b>
    <br>
    European Conference on Computer Vision (<strong>ECCV</strong>), 2024, <a style="color: red;">(Oral)</a><br>
    [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06174.pdf" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>

<div class="paper" id="paper"><img class="paper" src="./src/expert_sparsity.png" title="Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models">
  <div> <strong>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models
  </strong><br>
  <b>Xudong Lu*</b>, Qi Liu*, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, <b>Hongsheng Li</b><br>
  Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>), 2024</a><br>
  [<a href="https://arxiv.org/abs/2402.14800" target="_blank">Paper</a>]   
    <br>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="paper"><img class="paper" src="./src/spp_index.png" title="SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models">
    <div> <strong>SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models
    </strong><br>
    <b>Xudong Lu*</b>, Aojun Zhou*, Yuhui Xu*, Renrui Zhang, Peng Gao, <b>Hongsheng Li</b><br>
    International Conference on Machine Learning (<strong>ICML</strong>), 2024<br>
    [<a href="https://arxiv.org/abs/2405.16057" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>

  <div class="paper" id="paper"><img class="paper" src="./src/pangea.png" title="From isolated islands to pangea: Unifying semantic space for human action understanding">
    <div> <strong>From isolated islands to pangea: Unifying semantic space for human action understanding
    </strong><br>
    Yong-Lu Li*, Xiaoqian Wu*, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun Ji, Junyi Zhang, Yixing Li, <b>Xudong Lu</b>, Jingru Tan, <b>Cewu Lu</b><br>
    The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024, <a style="color: red;">(Highlight)</a><br>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_From_Isolated_Islands_to_Pangea_Unifying_Semantic_Space_for_Human_CVPR_2024_paper.pdf" target="_blank">Paper</a>]   
      <br>
    </div>
    <div class="spanner"></div>
    </div>


<div class="paper" id="paper"><img class="paper" src="./src/quantumdarts.png" title="QuantumDARTS: Differentiable Quantum Architecture Search for  Variational Quantum Algorithms">
  <div> <strong>QuantumDARTS: Differentiable Quantum Architecture Search for Variational Quantum Algorithms
</strong><br>
Wenjie Wu, Ge Yan, <b>Xudong Lu</b>, Kaisen Pan, <b>Junchi Yan</b><br>
International Conference on Machine Learning (<strong>ICML</strong>), 2023<br>
[<a href="https://proceedings.mlr.press/v202/wu23v/wu23v.pdf" target="_blank">Paper</a>]   
  <br>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="paper"><img class="paper" src="./src/qasbench.png" title="QAS-Bench: Rethinking Quantum Architecture Search and A Benchmark">
  <div> <strong>QAS-Bench: Rethinking Quantum Architecture Search and A Benchmark
  </strong><br>
  <b>Xudong Lu</b>, Kaisen Pan, Ge Yan, Jiaming Shan, Wenjie Wu, <b>Junchi Yan</b><br>
  International Conference on Machine Learning (<strong>ICML</strong>), 2023<br>
  [<a href="https://openreview.net/pdf?id=Fu8URyrpiY" target="_blank">Paper</a>]   
    <br>
  </div>
  <div class="spanner"></div>
  </div>

  <br>

  <div style="clear: both;">
    <div class="section">
    <h2>Educational Background</h2>

    <br>
    </h2>
    <div class="paper">
      <br>
  
      <table style="width: 100%; text-align: center; border: 0; cellpadding: 10;">
        <tbody>
          <tr>
            <td style="width: 25%; text-align: center;">
              <img src="./src/Emblem_of_CU.png" alt="CUHK" width="95" height="75">
            </td>
            <td style="width: 75%; vertical-align: top;text-align: left;">
              <p>
                <strong>Aug. 2023 - Present</strong>, Department of Electronic Engineering,
                  <strong>the Chinese University of Hong Kong</strong>
              </p>
              <br>
              <p>Ph.D. Student<br></p>
            </td>
          </tr>
        </tbody>
      </table>
      <br>
      <table style="width: 100%; text-align: center; border: 0; cellpadding: 10;">
        <tbody>
          <tr>
            <td style="width: 25%; text-align: center;">
              <img src="./src/SJTU_emblem.svg.png" alt="SJTU" width="75" height="75">
            </td>
            <td style="width: 75%; vertical-align: top;text-align: left;">
              <p>
                <strong>Aug. 2019 - Jun. 2023</strong>, Department of Computer Science and Engineering,
                  <strong>Shanghai Jiao Tong University</strong>
              </p>
              <br>
              <p>Bachelor<br></p>
            </td>
          </tr>
        </tbody>
      </table>

      <br>

    <br>
    
    </div>
    </div>
    </div>

<div style="clear: both;">
  <div class="section">
  <h2>Internship and Cooperation
  </h2><br>
  <div class="paper">
    <br>

<table>
  <tr>
    <td style='border: 10px; vertical-align: middle;'><img src="./src/pjlab.png" width="300px">&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td style='border: 10px; vertical-align: middle;'><img src="./src/vivo.png" width="120px">&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td style='border: 10px; vertical-align: middle;'><img src="./src/Huawei-Logo.png" width="160px"></td>
  </tr>
</table>
  <br>
  
  </div>
  </div>
  </div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Academic Service</h2><br>
<div class="paper">
<ul>
<strong>Journal Reviewer</strong> <br>
<p> IEEE TCSVT</p>
<br>
<strong>Conference Reviewer</strong> <br>
<p> CVPR, NeurIPS, ICLR, ICCV, ICML, AAAI</p>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;width:30%;margin:0 auto;">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=a&t=tt&d=biXEuO4WCCMPuW80XvQsWdlasay35RUvqSiXzLuUHAo&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
</div>

  <div class="footer-copyright center black-text">
    Copyright ¬© Lu Xudong, 2025
  </div>  
</footer>
<hr>
</body>
